<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
<head>
      <link rel="stylesheet" href="styles.css">
      <meta charset="UTF-8">
      <meta name="keywords" content="3D Shape learning">
      <meta name="author" content="Stefan Stojanov, Anh Thai">
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
</head>

<div class="header">
  <h1>Mobile and Computational Health</h1>
<!--  <p class="authors">
        <a href="https://anhthai1997.wordpress.com/">Zixuan Huang</a>*,
        <a href="https://anhthai1997.wordpress.com/">Anh Thai</a>*,
        <a href="https://sstojanov.github.io/">Stefan Stojanov</a>*,
        <a href="https://rehg.org/">James M. Rehg</a>
  </p>  -->
  <p class="institution">
        Rehg Lab, School of Interactive Computing, Georgia Institute of Technology
  </p>
<!--  <span class="footnote">
    * equal contribution
  </p> -->
</div>

<!-- <div class="links">

    <div class="gallery">
      <a target="_blank" href="https://github.com/devlearning-gt/3DShapeGen">
	<img src="img/code_img.png" alt="code">
      </a>
      <div class="desc">Code</div>
    </div>

    <div class="gallery">
      <a target="_blank" href="https://arxiv.org/abs/2006.07752">
	<img src="img/paper_img.png" alt="arxiv link">
      </a>
      <div class="desc">Paper <a href="/3DShapeGen/bibtex/3DShapeGen.bib">[BibTex]</a></div>
    </div>

</div> -->

<div class="content">

<!-- <h1> Abstract </h1>
<p>
A key challenge in single image 3D shape reconstruction is to ensure that deep models designed for single-view shape prediction can generalize to shapes which were not part of the training set. We find that such generalization to unseen categories of objects is a function of both architecture design and object pose representation during training. This paper introduces SDFNet, a novel shape prediction architecture and a training approach which supports effective generalization. We provide an extensive investigation of the factors that influence generalization accuracy and its measurement, ranging from the consistent use of 3D shape metrics to the choice of rendering parameters. We show that SDFNet provides state-of-the-art performance on seen and unseen shapes relative to existing baseline methods. We provide the first large-scale experimental evaluation of generalization performance on the complete ShapeNetCore.v2 dataset.
</p> -->

    <p>
	The development of effective treatments for chronic diseases, such as heart disease, diabetes, cancer, and Alzheimer’s, is a key global healthcare challenge. These conditions are primary drivers of healthcare spending and have a deleterious effect on quality of life and mortality. Fortunately, they are inextricably linked to modifiable health-related behavioral risk factors such as smoking, poor diet, and lack of physical activity. 
		
	Mobile health technology <a href="#MHBook">[Rehg-book]</a> provides a novel tool for supporting lifestyle modifications via behavioral interventions that can provide personalized guidance to patients, thereby reducing their risk factors and improving health outcomes. </p>
		
	<p>	
	While app-based therapies based on self-report data (i.e. EMA) are increasingly available, passive sensing from wearables holds the promise of continuously monitoring key physiological and contextual variables, thereby providing long-term insight into patterns of behavior and risk at low burden to participants.
		
	We are developing novel biomarkers from wearable sensors to capture physiological and contextual variables, and machine learning methods for modeling longitudinal data. We work with the <a href="https://mdot.md2k.org/">mDOT</a> and <a href="https://md2k.org/">MD2K</a> <a href="#MD2KJamia">[Kumar-JAMIA]</a> centers and have a broad set of collaborators in engineering, behavioral science, and statistics, with a focus on smoking, sedentary behavior, and substance abuse. We currently work in three research areas:

	  <ul>
   	    <li>Biomarkers for physiological and contextual risk factors</li>
      	  <li>Predictions of adverse outcomes from event data</li>
      	  <li>Modeling and imputing missing measurements in longitudinal data</li>
      </ul>
	  </p>

	  <!--   BIOMARKERS  -->

  <h1> Biomarkers for physiological and contextual risk factors</h1>

	  <p> We are collaborating with the <a href="https://irl.gatech.edu/">Inan Lab</a> in developing novel biomarkers for hemodynamic parameters, such as pulmonary capillary wedge pressure, and in developing predictive models for heart failure (HF) clinical status (i.e. compensated vs decompensated). Decompensation in HF is a key driver of rehospitalization, resulting in increased mortality risk and cost of care. Using an implantable device to measure pulmonary artery pressure and titrate treatment has been shown to reduce the risk of decompensation, but the procedure is invasive, expensive, and unsuitable for some patients. We are exploring ballistocardiography and seismocardiography as a means to estimate heart failure status by measuring and analyzing the small vibrations that propagate through the body due to the mechanical action of the heart. Ballistocardiography (BCG) uses a modified weighing scale, and we have demonstrated classification of HF decompensation at an AUC of 0.78 using BCG data collected in a home environment <a href="#BCG">[Burak-TBME]</a>. Seismocardiography uses an accelerometer placed on the chest and is currently under investigation.</p>
	  
	  <figure>
	     <img src="img/scg_example.png" alt="">    <!-- style="width:90%"  width="1877" height="777" -->
		  <figcaption>Seismocardiography (SCG) signals from a wearable patch attached to the sternum. ECG and 3-axis accelerometery capture the electrical and mechanical signatures of heart beats</figcaption>
	  </figure>

	  <p>
	  We are also developing biomarkers for contextual variables based on passively-collected signals from wearable sensors. We have collaborated with the MD2K center in developing a biomarker for smoking opportunity, which captures the situational factors (such as proximity to locations in which smoking is permitted) which can precipitate relapse during a quit attempt <a href="#Context">[Moreno-IMWUT]</a>. We have also developed a novel biomarker that uses a wearable camera to quantify exposure to screens, a known risk factor for sedentary behavior <a href="#TV">[Yun-TV]</a>. Below we illustrate several examples of attention to TVs and mobile devices inferred from wearable camera video (without eye tracking). A key challenge in contextual sensing is the accurate synchronization of multiple wearable modalities, and in particular wearable cameras, to a common timeline. We have developed an automatic synchronization approach, known as SyncWISE, and have demonstrated its effectiveness in aligning video and accelerometry signals <a href="#Sync">[Yun-Synch]</a>.</p>
	  
	  <div class="gifs">
          <div class="container">
              <div class="gallery">
                 <img class="gif" src="http://drive.google.com/uc?export=view&id=1OMwDPu1vMeDplpmXkzKbcbww2g3KvsqQ" >
                 <div class="desc">Plane</div>
              </div>
            <div class="gallery">
               <img class="gif" src="http://drive.google.com/uc?export=view&id=1LtPrzfP0U5pMGXZnCRJR6HIrqRrybQpc" >
               <div class="desc">Cup</div>
            </div>
          <!-- </div>
          <div class="container"> -->

            <div class="gallery">
               <img class="gif" src="http://drive.google.com/uc?export=view&id=1ARuGSONsHcasfyIUXfcKrEmrddJ8AUZT" >
               <div class="desc">Truck</div>

            </div>
            <div class="gallery">
              <img class="gif" src="http://drive.google.com/uc?export=view&id=1KVpeB0-HMnPrC-Fq1lYKMYqnLh0feQAk" >
              <div class="desc">Bee</div>

           </div>
         </div>
	  </div>
	  <div class="desc">Examples of automatic detection of attention to screens from head-mounted camera video</div>
	  	  
	  <!--    Predictions of adverse outcomes from event data -->

	  <h1> Predictions of adverse outcomes from event data </h1>

	  <p>
	  Many common temporal data sources in clinical and mobile health applications consist of event data, in which measurements occur at arbitrary, irregularly-distributed time points. Visits to the clinic for assessment, which are common in treating conditions such as glaucoma or autism, rarely occur on a fixed schedule. Likewise, in mobile health applications it is common to perform Ecological Momentary Assessments (EMA) at randomly-chosen times throughout the day. We have developed two approaches to modeling such data: 

	  <ul>
   	      <li>Latent state models based on Continuous Time HMM (CT-HMM)</li>
      	  <li>Deep learning models based on transformers</li>
      </ul>
	  </p>

	  <p>
	  A CT-HMM is an HMM in which both the transitions between hidden states and the arrival of observations can occur at arbitrary (continuous) times, rather than on a fixed clock. We have developed efficient parameter learning methods for CT-HMMs <a href="#CTnips">[Liu-NeurIPS]</a> and have demonstrated their utility in modeling the progression of glaucoma and Alzheimer’s disease (illustrated below). We have developed a spatiotemporal CT-HMM approach to modeling glaucoma progression, which can describe the temporal history of 2D visual field and retinal thickness maps <a href="#CTmlhc">[Supriya-ML4HC]</a> and accurately predict future patient status as a tool for treatment management. We have developed a predictive model for relapse in substance abuse by combining a CT-HMM model of latent behavioral states with a survival model for risk prediction <a href="#CTicml">[Moreno-iSurvive]</a>.</p>
		  
    <!-- <b>ADD A FEW MORE DETAILS FROM OUR PAPER</b>. -->

    <figure>
       <img src="img/disease_prog.png" alt="">    <!-- style="width:90%"  width="1877" height="777" -->
      <figcaption> Decoding latent state trajectories of the progression of Glaucoma and Alzheimer's Disease</figcaption>
    </figure>

    <p>

    We are currently developing transformer-based models for predicting EMA noncompliance. It is extremely common and useful to gather self-reported data from participants, for example as EMAs in behavioral studies and as electronic patient-reported outcomes (ePRO) in clinical trials. Noncompliance, in which participants fail to provide data at select time points, is a significant and well-recognized problem. We are developing a deep learning architecture which analyzes a sequence of EMA responses, along with other contextual cues, and predicts the likelihood that a participant will be noncompliant to the next EMA prompt. Our investigation leverages a novel dataset in which participant availability is automatically-assessed from sensor data prior to triggering EMA prompts, making it possible to distinguish noncompliance from overall nonresponse. We have shown that it is possible to predict noncompliance at an AUC of 0.76. </p>

    <figure>
       <img src="img/compliance.png" alt="">    <!-- style="width:90%"  width="1877" height="777" -->
      <figcaption> Analyzing compliance of EMAs with transformer-based architectures </figcaption>
    </figure>


	  <!--   Modeling and imputing missing measurements in longitudinal data -->

	  <h1> Modeling and imputing missing measurements in longitudinal data
 </h1>
	  <p>
    Problems with missing data are endemic to clinical and mobile health applications. We are developing principled strategies for imputing missing observations from temporal data in the context of the hierarchical computations that naturally arise in mHealth biomarker inference. As a first step along these lines, we have addressed the problem of missing observations in panel count data. In many mHealth applications, participants are asked to self-report counts of behaviors of interest (e.g. number of cigarettes smoked since the last self-assessment). This is an example of a more general setting of modeling counting processes for adverse behavioral outcomes. We have recently addressed the problem of modeling the mean function of a counting process from panel count data with missing counts [Moreno-NeurIPS]. We developed a simple functional EM algorithm to wrap existing panel count data methods, and extended finite sample parametric EM theory to the functional setting via a novel use of Gateaux derivatives. Our results suggest that the standard imputation methods may be under-estimating actual counts.

	  </p>

    <figure>
       <img src="img/smoking_cess.png" alt="">    <!-- style="width:90%"  width="1877" height="777" -->
      <figcaption> Functional EM algorithm to predict missingness of temporal data </figcaption>
    </figure>

	 
	  <!--   MEMBERS AND COLLABORATORS -->

	  <h1> Personnel </h1>
	  
	  <h2> Georgia Tech </h2>
          <div class="personnel_gallery">

                <div class="column">
                         <a href="https://www.linkedin.com/in/varol-burak-aydemir-80013ba6">
                         <img src="gt_personnel/burak.jpg" >
                         </a>
                         <figcaption> Varol Burak Aydemir </figcaption>
                </div>
                <div class="column">
                         <a href="https://boostedml.com/">
                         <img src="gt_personnel/alex.jpg" >
                         </a>
                         <figcaption> Alexander Moreno </figcaption>
                </div>
                <div class="column">
                         <a href="https://supriyanagesh94.github.io/">
                         <img src="gt_personnel/supriya.jpg" >
                         </a>
                         <figcaption> Supriya Nagesh  </figcaption>
                </div>

	  </div>

              <!-- add institution -->
          <div class="personnel_gallery">
                <div class="column">
                         <a href="https://maxxu05.github.io/">
                         <img src="gt_personnel/max.jpg" >
                         </a>
                         <figcaption> Max Xu </figcaption>
                         <a href="">
                </div>
                <div class="column">
                         <a href="">
                         <img src="gt_personnel/yun.jpg" >
                         </a>
                         <figcaption> Yun Zhang </figcaption>
                </div>
                <div class="column">
                         <a href="https://rehg.org/">
                         <img src="gt_personnel/jim.jpg" >
                         </a>
                         <figcaption> <a href="https://rehg.org/">  Dr. James M. Rehg </a> </figcaption>
                </div>

	  </div>

<!-- 	  <h2> External Collaborators </h2>
          <div class="personnel_gallery">

                <div class="column">
                         <a href="https://scholar.google.com/citations?user=IuINaBgAAAAJ&hl=en">
                         <img src="ext_personnel/elizabeth_clerkin.jpg" >
                         </a>
                         <figcaption> Elizabeth Clerkin </br> (Indiana Univ.) </figcaption>
                </div>
                <div class="column">
                         <a href="https://ieeexplore.ieee.org/author/37086031826">
                         <img src="ext_personnel/elizabeth_hart.jpg" >
                         </a>
                         <figcaption> Elizabeth Hart </br> (Indiana Univ.) </figcaption>
                </div>
                <div class="column">
                         <a href="https://psych.indiana.edu/directory/faculty/smith-linda.html">
                         <img src="ext_personnel/linda_smith.jpg" >
                         </a>
                         <figcaption> Dr. Linda B. Smith </br> (Indiana Univ.) </figcaption>
                </div>
                <div class="column">
                         <a href="https://liberalarts.utexas.edu/psychology/faculty/cy2856">
                         <img src="ext_personnel/chen_yu.jpg" >
                         </a>
                         <figcaption> Dr. Chen Yu </br> (UT Austin) </figcaption>
                </div>

	  </div> -->


	  <!--   PROJECT FUNDING -->

	  <h1> Project Funding </h1>
	  <ul>
	  	  <li> NSF Awards <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=1320348">RI 1320348</a>,
		  	   <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=1524565">BCS 1524565</a>,
			   <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=1936970">RAISE 1936970</a></li>
	      <li> Gifts from Xerox PARC and Intel Research</li>
		  </ul>


		  <!--
<p>
  When evaluated on 3 Degree-of-Freedom Viewer Centered (3 DOF VC)&mdash;object pose varies along azimuth, elevation and tilt,
  our empirical findings show marginal decrease in performance between seen and unseen classes for the 3 DOF VC model.
  This is new evidence that it is possible to learn a general shape representation with correct depth estimation and 3 DOF VC training.
  </p>
<center><img src="img/representation_img.png" ></center>
<div class="desc"> Quantitative evaluation of 3 DOF VC shows high performance on both seen and unseen categories. Model is trained on ground truth depth and normal images.</div>

<h1> Generalization Across Different Datasets </h1>
<center><img src="img/abc_shapenet_img.png" ></center>
<div class="desc"> Sample images of our renders of the four most common ShapeNet categories and of objects from ABC. It is evident that the two datasets have different shape properties.</div>

<p>
To further test the generalization ability of SDFNet, we train it one one shape dataset and test it on a significantly different shape dataset. Our findings show that when trained on ABC and tested on the 42 unseen categories of ShapeNet, 3 DOF VC SDFNet obtains comparable performance to SDFNet trained on the 13 ShapeNet categories. SDFNet trained on ShapeNet performs relatively worse when tested on ABC.
</p>
<center><img src="img/shapenet_abc_PRED_img.png" ></center>
<div class="desc"> Qualitative comparison of models trained on ABC and tested on ShapeNet and vice-versa. Note the good reconstruction quality on the occluded part of the object.</div>

		  -->

<h1> References  </h1>
<ol>
    <li><a name="CLRep">Stojanov, S.</a>, Mishra, S., Thai, N. A., Dhanda, N., Humayun, A., Yu, C., Smith, L. B., and Rehg, J. M. (2019).
		<a href="https://iolfcv.github.io/">Incremental Object Learning from Contiguous Views.</a> In <i>Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR 19)</i>.</li>
	<li><a name="SDFnet">Thai, A.</a>, Stojanov, S., Upadhya, V., Rehg, J. M. (2020).
		<a href="https://devlearning-gt.github.io/3DShapeGen/">3D Reconstruction of Novel Object Shapes from Single Images.</a> <cite>arXiv preprint:2006.07752.</cite></li>
	<li><a name="EarlyStat">Clerkin, E. M.</a>, Hart, E., Rehg, J. M., Yu, C., & Smith, L. B. (2017).
		<a href="https://royalsocietypublishing.org/doi/full/10.1098/rstb.2016.0055">Real-world visual statistics and infants’ first-learned object names.</a>
		<i>Phil. Trans. of the Royal Society B: Biological Sciences</i>, 372(20160055), 1–10.</li>
	<li><a name="IterTeach">Liu, W.</a>, Dai, B., Humayun, A., Tay, C., Yu, C., Smith, L. B., Rehg, J. M., and Song, L. (2017).
		<a href="https://arxiv.org/abs/1705.10470">Iterative Machine Teaching.</a> In <i>Proc. 34th Intl. Conf. on Machine Learning (ICML 17)</i> (p. PMLR 70:2149-2158).</li>

<!--
		  <li><a name="GenRe">Zhang, X., Zhang, Z., Zhang, C., Tenenbaum, J., Freeman, B., & Wu, J. (2018). Learning to reconstruct shapes from unseen classes.
      In <cite>Advances in Neural Information Processing Systems </cite>(pp. 2257-2268).</a></li>
    <li><a name="OccNet">Mescheder, L., Oechsle, M., Niemeyer, M., Nowozin, S., & Geiger, A. (2019).
      Occupancy networks: Learning 3d reconstruction in function space. In <cite>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition </cite>(pp. 4460-4470).</a></li>
	  -->
  </ol>

</div>
